{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Module 2 Code Examples - File I/O",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Read data from file",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with open('iris.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Let's take a deeper look into what we are getting out\nwith open('iris.csv', 'r') as file:\n    reader = csv.reader(file)\n    next(reader, None) #skips over the first (header) row so we can see what this looks like for the actual data\n    for row in reader:\n        print(row)\n        print(type(row)) # each row will be a list\n        print(row[0],type(row[0])) # each element is a string, even if the value looks numeric!\n        break # exits the loop",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### Side note\n Why don't we just read all the data in at once? As you get more into data science, you will often encounter very large csv files that cannot necessarily be read all at once without causing out of memory errors.  While this would not be a problem here, the convention many data scientists follow is to always read in lines one at a time when working with csv data.\n\nHowever, what if we wanted to read in data from the full file at once? In that case, we could simply use the .read() command instead of using the .csv library, but this also makes it harder to parse the data.\n\n```\nwith open('iris.csv', 'r') as file:\n    contents = file.read()\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Write Data to a File",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with open('iris.csv', 'a',newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow(['5.1', '3.5', '1.4', '.2', 'TEST'])\n\nwith open('iris.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Files as Dictionaries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with open('iris.csv', 'r') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        print(row) #headers are automatically mapped to values!",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
